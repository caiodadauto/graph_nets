{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xUTIiPkb45xH",
    "outputId": "d96a929a-2eb3-40b5-b2e6-db0508e1f289"
   },
   "outputs": [],
   "source": [
    "#@title ### Install the Graph Nets library on this Colaboratory runtime  { form-width: \"60%\", run: \"auto\"}\n",
    "#@markdown <br>1. Connect to a local or hosted Colaboratory runtime by clicking the **Connect** button at the top-right.<br>2. Choose \"Yes\" below to install the Graph Nets library on the runtime machine with:<br>\n",
    "\n",
    "install_graph_nets_library = \"Yes\"  #@param [\"Yes\", \"No\"]\n",
    "install_pybrite = \"Yes\"  #@param [\"Yes\", \"No\"]\n",
    "install_tf_gpu = \"No\"  #@param [\"Yes\", \"No\"]\n",
    "\n",
    "print(\"Installing Tensorflow library with:\")\n",
    "print(\"  $ pip install tensorflow=='r1.14'\\n\")\n",
    "print(\"Output message from command:\\n\")\n",
    "#!pip uninstall tensorflow tensorflow-gpu\n",
    "!pip install tensorflow==\"1.14.0\"\n",
    "\n",
    "if install_graph_nets_library.lower() == \"yes\":\n",
    "    print(\"Installing variation of Graph Nets library from github with:\")\n",
    "    print(\"  $ git clone https://github.com/caiodadauto/graph_nets\\n\")\n",
    "    print(\"  $ pip install graph_nets/\\n\")\n",
    "    print(\"Output message from command:\\n\")\n",
    "    !git clone https://github.com/caiodadauto/graph_nets\n",
    "    !pip install graph_nets/\n",
    "else:\n",
    "    print(\"Skipping installation of Graph Nets library\")\n",
    "    \n",
    "if install_tf_gpu.lower() == \"yes\":\n",
    "    print(\"Installing Tensorflow GPU library with:\")\n",
    "    print(\"  $ pip install tensorflow-gpu\\n\")\n",
    "    print(\"Output message from command:\\n\")\n",
    "    !pip install tensorflow-gpu\n",
    "else:\n",
    "    print(\"Skipping installation of Tensorflow GPU library\")\n",
    "    \n",
    "if install_pybrite.lower() == \"yes\":\n",
    "    print(\"Installing pybrite from github with:\")\n",
    "    print(\"  $ git clone --single-branch --branch colab https://github.com/caiodadauto/pybrite.git\\n\")\n",
    "    print(\"  $ pip install pybrite/pybrite/\")\n",
    "    print(\"Output message from command:\\n\")\n",
    "    !git clone --single-branch --branch colab https://github.com/caiodadauto/pybrite.git\n",
    "    !pip install pybrite/pybrite/\n",
    "else:\n",
    "    print(\"Skipping installation of Pybrite library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "rY_mZraR45xT",
    "outputId": "be9742be-d4c9-4cc7-cac1-d2ed2cbb22ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1016 01:52:31.529383 140297052448576 deprecation_wrapper.py:119] From /home/caio/.local/lib/python3.7/site-packages/sonnet/python/custom_getters/restore_initializer.py:27: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W1016 01:52:32.017647 140297052448576 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1016 01:52:32.129194 140297052448576 deprecation_wrapper.py:119] From /home/caio/.local/lib/python3.7/site-packages/graph_nets/blocks.py:478: The name tf.unsorted_segment_sum is deprecated. Please use tf.math.unsorted_segment_sum instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Imports  { form-width: \"30%\" }\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pybrite\n",
    "from graph_nets import graphs\n",
    "from graph_nets import blocks\n",
    "from graph_nets import modules\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "ckW56Fik45xd",
    "outputId": "981d2b88-8db0-41f2-d4a3-0182e10d0e89"
   },
   "outputs": [],
   "source": [
    "#@title Set google drive access  { form-width: \"30%\" }\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Dad6eRI45xk"
   },
   "outputs": [],
   "source": [
    "#@title Set Seed, Layers Number and their Size { form-width: \"30%\" }\n",
    "\n",
    "SEED = 2  #@param{type: 'integer'}\n",
    "NUM_LAYERS = 2  #@param{type: 'integer'}\n",
    "LATENT_SIZE = 16  #@param{type: 'integer'}\n",
    "TEST_LOCAL_STATS = False\n",
    "SCALE = True\n",
    "DRIVE_PATH = \"/content/gdrive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eEsbweX45xr"
   },
   "outputs": [],
   "source": [
    "#@title Helper Functions: Tensorflow Integration { form-width: \"30%\" }\n",
    "\n",
    "def create_placeholders(batch_generator):\n",
    "    # Create some example data for inspecting the vector sizes.\n",
    "    input_graphs, target_graphs, _ = next(batch_generator)\n",
    "    input_ph = utils_tf.placeholders_from_networkxs(input_graphs)\n",
    "    target_ph = utils_tf.placeholders_from_networkxs(target_graphs)\n",
    "    \n",
    "    dtype = tf.as_dtype(utils_np.networkxs_to_graphs_tuple(target_graphs).edges.dtype)\n",
    "    weight_ph = tf.placeholder(dtype, name=\"loss_weights\")\n",
    "    is_training_ph = tf.placeholder(tf.bool, name=\"training_flag\")\n",
    "    return input_ph, target_ph, weight_ph, is_training_ph\n",
    "\n",
    "def create_feed_dict(batch_generator, is_training, weights, input_ph, target_ph, is_training_ph, weight_ph):\n",
    "    inputs, targets, pos = next(batch_generator)\n",
    "    input_graphs = utils_np.networkxs_to_graphs_tuple(inputs)\n",
    "    target_graphs = utils_np.networkxs_to_graphs_tuple(targets)\n",
    "    \n",
    "    batch_weights = np.ones_like(target_graphs.edges)\n",
    "    batch_weights[target_graphs.edges == 0] *= weights[0]\n",
    "    batch_weights[target_graphs.edges == 1] *= weights[1]\n",
    "        \n",
    "    feed_dict = {input_ph: input_graphs, target_ph: target_graphs, is_training_ph: is_training, weight_ph: batch_weights}\n",
    "    return feed_dict, pos\n",
    "\n",
    "def get_routing_link(n_nodes, senders, edges):\n",
    "    nodes = range(n_nodes)\n",
    "    for n in nodes:\n",
    "        #print(\"Node {}\".format(n))\n",
    "        edges_mask = np.argwhere(senders == n)\n",
    "        node_edges = edges[edges_mask]\n",
    "        #print(node_edges)\n",
    "        indices = np.arange(len(node_edges))\n",
    "        link_idx = np.argmax(node_edges)\n",
    "        node_edges[link_idx] = 1.\n",
    "        node_edges[indices[indices != link_idx]] = 0.\n",
    "        #print(node_edges)\n",
    "        edges[edges_mask] = node_edges\n",
    "    return edges\n",
    "    \n",
    "def compute_accuracy(target, output, distribution=False):\n",
    "    acc_all = []\n",
    "    solved_all = []\n",
    "    acc_true_all = []\n",
    "    acc_false_all = []\n",
    "    solved_true_all = []\n",
    "    solved_false_all = []\n",
    "    \n",
    "    tg_dict = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    out_dict = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    for tg_graph, out_graph in zip(tg_dict, out_dict):\n",
    "        expect = tg_graph[\"edges\"].reshape(-1)\n",
    "        predict = get_routing_link(\n",
    "            out_graph[\"n_node\"], out_graph[\"senders\"], out_graph[\"edges\"]).reshape(-1)\n",
    "        print(expect)\n",
    "        print(out_graph[\"edges\"].reshape(-1))\n",
    "        true_mask = np.ma.masked_equal(expect, 1).mask\n",
    "        false_mask = np.ma.masked_equal(expect, 0).mask\n",
    "\n",
    "        acc = (expect == predict)\n",
    "        acc_true = acc[true_mask]\n",
    "        acc_false = acc[false_mask]\n",
    "\n",
    "        solved = np.all(acc)\n",
    "        solved_true = np.all(acc_true)\n",
    "        solved_false = np.all(acc_false)\n",
    "\n",
    "        acc_all.append(np.mean(acc))\n",
    "        acc_true_all.append(np.mean(acc_true))\n",
    "        acc_false_all.append(np.mean(acc_false))\n",
    "        \n",
    "        solved_all.append(solved)\n",
    "        solved_true_all.append(solved_true)\n",
    "        solved_false_all.append(solved_false)\n",
    "    acc_all = np.stack(acc_all)\n",
    "    acc_true_all = np.stack(acc_true_all)\n",
    "    acc_false_all = np.stack(acc_false_all)\n",
    "\n",
    "    solved_all = np.stack(solved_all)\n",
    "    solved_true_all = np.stack(solved_true_all)\n",
    "    solved_false_all = np.stack(solved_false_all)\n",
    "    if not distribution:\n",
    "        acc_all = np.mean(acc_all)\n",
    "        acc_true_all = np.mean(acc_true_all)\n",
    "        acc_false_all = np.mean(acc_false_all)\n",
    "\n",
    "        solved_all = np.mean(solved_all)\n",
    "        solved_true_all = np.mean(solved_true_all)\n",
    "        solved_false_all = np.mean(solved_false_all)\n",
    "    return acc_all, solved_all, acc_true_all, solved_true_all, acc_false_all, solved_false_all\n",
    "\n",
    "def get_generator_path_metrics(inputs, targets, outputs):\n",
    "    out_dicts = utils_np.graphs_tuple_to_data_dicts(outputs)\n",
    "    in_dicts = utils_np.graphs_tuple_to_data_dicts(inputs)\n",
    "    tg_dicts = utils_np.graphs_tuple_to_data_dicts(targets)\n",
    "\n",
    "    n_graphs = len(tg_dicts)\n",
    "    for tg_graph, out_graph, in_graph, idx_graph in zip(tg_dicts, out_dicts, in_dicts, range(n_graphs)):\n",
    "        n_node = out_graph[\"n_node\"]\n",
    "        tg_graph_dist = tg_graph[\"nodes\"][:,0]\n",
    "        tg_graph_hops = tg_graph[\"nodes\"][:,1]\n",
    "        out_graph_dist = np.zeros_like(tg_graph_dist)\n",
    "        out_graph_hops = np.zeros_like(tg_graph_dist)\n",
    "        end_node = np.argwhere(tg_graph_dist == 0).reshape(1)[0]\n",
    "        for node in range(n_node):\n",
    "            hops = 0\n",
    "            strength = 0\n",
    "            start = node\n",
    "            sender = None\n",
    "            reachable = True\n",
    "            path = np.zeros(n_node, dtype=np.bool)\n",
    "            while start != end_node:\n",
    "                path[start] = True\n",
    "                start_edges_idx = np.argwhere(out_graph[\"senders\"] == start)\n",
    "                receivers = out_graph[\"receivers\"][start_edges_idx]\n",
    "                start_edges = out_graph[\"edges\"][start_edges_idx]\n",
    "                remove_sender = (receivers != sender) if sender else np.ones_like(receivers, dtype=np.bool)\n",
    "                edge_forward_idx = np.argmax(start_edges[remove_sender])\n",
    "\n",
    "                if edge_forward_idx.size > 1:\n",
    "                    print(\"\\nMore than one max prob\\n\")\n",
    "\n",
    "                sender = start\n",
    "                start = receivers[remove_sender][edge_forward_idx]\n",
    "                \n",
    "                if path[start]:\n",
    "                    reachable = False\n",
    "                    break\n",
    "                    \n",
    "                hops += 1\n",
    "                strength += in_graph[\"edges\"][start_edges_idx][remove_sender][edge_forward_idx][0]\n",
    "            if reachable:\n",
    "                out_graph_dist[node] = strength\n",
    "                out_graph_hops[node] = hops\n",
    "        out_graph_hops = np.delete(out_graph_hops, end_node)\n",
    "        out_graph_dist = np.delete(out_graph_dist, end_node)\n",
    "        tg_graph_hops = np.delete(tg_graph_hops, end_node)\n",
    "        tg_graph_dist = np.delete(tg_graph_dist, end_node)\n",
    "        idx_non_zero = np.flatnonzero(out_graph_hops)\n",
    "        unreachable_p =  1 - idx_non_zero.size / out_graph_dist.size\n",
    "        if idx_non_zero.size > 0:\n",
    "            diff_dist = (np.abs(out_graph_dist[idx_non_zero] - tg_graph_dist[idx_non_zero]))\n",
    "            diff_hops = (np.abs(out_graph_hops[idx_non_zero] - tg_graph_hops[idx_non_zero]))\n",
    "            yield (diff_dist, diff_hops, unreachable_p)\n",
    "        else:\n",
    "            yield (None, None, unreachable_p)\n",
    "\n",
    "def aggregator_path_metrics(inputs, targets, outputs, distribution=False):\n",
    "    n_graphs = targets.n_node.size\n",
    "    idx_graph = 0\n",
    "    none_idx = []\n",
    "    hist_hops = []\n",
    "    hist_dist = []\n",
    "    batch_max_dist_diff = np.zeros(n_graphs)\n",
    "    batch_min_dist_diff = np.zeros(n_graphs)\n",
    "    batch_avg_dist_diff = np.zeros(n_graphs)\n",
    "    batch_max_hops_diff = np.zeros(n_graphs)\n",
    "    batch_min_hops_diff = np.zeros(n_graphs)\n",
    "    batch_avg_hops_diff = np.zeros(n_graphs)\n",
    "    batch_unreachable_p = np.zeros(n_graphs)\n",
    "    metrics_graph_generator = get_generator_path_metrics(inputs, targets, outputs)\n",
    "    for diff_dist, diff_hops, unreachable_p in metrics_graph_generator:\n",
    "        batch_unreachable_p[idx_graph] = unreachable_p\n",
    "        \n",
    "        if np.any(diff_dist == None):\n",
    "            none_idx.append(idx_graph)\n",
    "        else:\n",
    "            batch_max_dist_diff[idx_graph] = np.max(diff_dist)\n",
    "            batch_min_dist_diff[idx_graph] = np.min(diff_dist)\n",
    "            batch_avg_dist_diff[idx_graph] = np.mean(diff_dist)\n",
    "            batch_max_hops_diff[idx_graph] = np.max(diff_hops)\n",
    "            batch_min_hops_diff[idx_graph] = np.min(diff_hops)\n",
    "            batch_avg_hops_diff[idx_graph] = np.mean(diff_hops)\n",
    "            if distribution:\n",
    "                hist_hops.append(diff_hops)\n",
    "                hist_dist.append(diff_dist)\n",
    "        idx_graph += 1\n",
    "    batch_max_dist_diff = np.delete(batch_max_dist_diff, none_idx)\n",
    "    batch_min_dist_diff = np.delete(batch_min_dist_diff, none_idx)\n",
    "    batch_avg_dist_diff = np.delete(batch_avg_dist_diff, none_idx)\n",
    "    batch_max_hops_diff = np.delete(batch_max_hops_diff, none_idx)\n",
    "    batch_min_hops_diff = np.delete(batch_min_hops_diff, none_idx)\n",
    "    batch_avg_hops_diff = np.delete(batch_avg_hops_diff, none_idx)\n",
    "    if not distribution:\n",
    "        return dict(avg_batch_max_dist_diff=np.mean(batch_max_dist_diff) if batch_max_dist_diff.size else np.infty,\n",
    "                    avg_batch_min_dist_diff=np.mean(batch_min_dist_diff) if batch_min_dist_diff.size else np.infty,\n",
    "                    avg_batch_avg_dist_diff=np.mean(batch_avg_dist_diff) if batch_avg_dist_diff.size else np.infty,\n",
    "                    avg_batch_max_hops_diff=np.mean(batch_max_hops_diff) if batch_max_hops_diff.size else np.infty,\n",
    "                    avg_batch_min_hops_diff=np.mean(batch_min_hops_diff) if batch_min_hops_diff.size else np.infty,\n",
    "                    avg_batch_avg_hops_diff=np.mean(batch_avg_hops_diff) if batch_avg_hops_diff.size else np.infty,\n",
    "                    max_batch_unreachable_p=np.max(batch_unreachable_p),\n",
    "                    min_batch_unreachable_p=np.min(batch_unreachable_p),\n",
    "                    avg_batch_unreachable_p=np.mean(batch_unreachable_p))\n",
    "    else:\n",
    "        return {\"percentage of unreachable paths\":batch_unreachable_p, \"difference of hops\":np.concatenate(hist_hops), \"difference of strength\":np.concatenate(hist_dist)}\n",
    "\n",
    "def mse(n_node, tg, out, weights):\n",
    "    op1 = tf.square(tg - out)\n",
    "    op2 = tf.multiply(weights, op1)\n",
    "    op3 = tf.reduce_sum(op2)\n",
    "    n_graph = tf.cast(tf.count_nonzero(n_node), dtype=op3.dtype)\n",
    "    return tf.divide(op3, n_graph)\n",
    "    \n",
    "def create_loss_ops(target_op, output_ops, weights):\n",
    "    loss_ops = [\n",
    "        mse(target_op.n_node, target_op.edges, output_op.edges, weights)\n",
    "        for output_op in output_ops\n",
    "    ]\n",
    "    return loss_ops\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "    \"\"\"Lets an iterable of TF graphs be output from a session as NP graphs.\"\"\"\n",
    "    return [utils_tf.make_runnable_in_session(a) for a in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPE4S4Mf45xw"
   },
   "outputs": [],
   "source": [
    "#@title Helper Functions: Create Layers { form-width: \"30%\" }\n",
    "\n",
    "class LeakyReluMLP(snt.AbstractModule):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 n_layers,\n",
    "                 name=\"LeakyReluNormMLP\"):\n",
    "        super(LeakyReluMLP, self).__init__(name=name)\n",
    "        self._n_layers = n_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        with self._enter_variable_scope():\n",
    "            self._linear_layers = []\n",
    "            for _ in range(self._n_layers):\n",
    "                self._linear_layers.append(snt.Linear(self._hidden_size))\n",
    "            \n",
    "    def _build(self, inputs, is_training):\n",
    "        outputs_op = inputs\n",
    "        for linear in self._linear_layers:\n",
    "            outputs_op = linear(outputs_op)\n",
    "            outputs_op = tf.nn.leaky_relu(outputs_op, alpha=0.05)\n",
    "        return outputs_op\n",
    "\n",
    "\n",
    "class LeakyReluNormMLP(snt.AbstractModule):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 n_layers,\n",
    "                 name=\"LeakyReluNormMLP\"):\n",
    "        super(LeakyReluNormMLP, self).__init__(name=name)\n",
    "        self._n_layers = n_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        with self._enter_variable_scope():\n",
    "            self._linear_layers = []\n",
    "            self._bn_layers = []\n",
    "            for _ in range(self._n_layers):\n",
    "                self._linear_layers.append(snt.Linear(self._hidden_size))\n",
    "                self._bn_layers.append(snt.BatchNorm(scale=SCALE))\n",
    "            \n",
    "    def _build(self, inputs, is_training):\n",
    "        outputs_op = inputs\n",
    "        for linear, bn in zip(self._linear_layers, self._bn_layers):\n",
    "            outputs_op = linear(outputs_op)\n",
    "            outputs_op = tf.nn.leaky_relu(outputs_op, alpha=0.05)\n",
    "            outputs_op = bn(outputs_op, is_training=is_training, test_local_stats=TEST_LOCAL_STATS)\n",
    "        return outputs_op\n",
    "    \n",
    "class LeakyReluNormGRU(snt.AbstractModule):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 recurrent_dropout=0.75,\n",
    "                 name=\"LeakyReluNormGRU\"):\n",
    "        super(LeakyReluNormGRU, self).__init__(name=name)\n",
    "        self._hidden_size = hidden_size\n",
    "        with self._enter_variable_scope():\n",
    "            self._gru = snt.GRU(self._hidden_size)\n",
    "            self._dropout_gru = snt.python.modules.gated_rnn.RecurrentDropoutWrapper(self._gru, recurrent_dropout)\n",
    "            self._batch_norm = snt.BatchNorm(scale=SCALE)\n",
    "    \n",
    "    def get_initial_state(self, batch_size, dtype=tf.float64):\n",
    "        return self._dropout_gru.initial_state(batch_size, dtype=dtype)\n",
    "    \n",
    "    def _build(self, inputs, prev_states, is_training):\n",
    "        def true_fn():\n",
    "            return self._dropout_gru(inputs, prev_states)\n",
    "        \n",
    "        def false_fn():\n",
    "            o, ns = self._gru(inputs, prev_states[0])\n",
    "            ns = (ns, [tf.ones_like(ns, name=\"FoolMask\")])\n",
    "            return o, ns\n",
    "        \n",
    "        outputs_op, next_states = tf.cond(is_training, true_fn=true_fn, false_fn=false_fn)\n",
    "        outputs_op = tf.nn.leaky_relu(outputs_op, alpha=0.05)\n",
    "        outputs_op = self._batch_norm(outputs_op, is_training=is_training, test_local_stats=TEST_LOCAL_STATS)\n",
    "        return outputs_op, next_states\n",
    "\n",
    "def make_gru_model(size=LATENT_SIZE):\n",
    "    \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "\n",
    "    The parameters of each new MLP are not shared with others generated by\n",
    "    this function.\n",
    "\n",
    "    Returns:\n",
    "    A Sonnet module which contains the MLP and LayerNorm.\n",
    "    \"\"\"\n",
    "    return LeakyReluNormGRU(size)\n",
    "\n",
    "def make_mlp_model(size=LATENT_SIZE, n_layers=NUM_LAYERS, model=LeakyReluNormMLP):\n",
    "    \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "\n",
    "    The parameters of each new MLP are not shared with others generated by\n",
    "    this function.\n",
    "\n",
    "    Returns:\n",
    "    A Sonnet module which contains the MLP and LayerNorm.\n",
    "    \"\"\"\n",
    "    return model(size, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6efgaON45x2"
   },
   "outputs": [],
   "source": [
    "#@title Helper Classes: Modules to Integrate the Encode-Process-Decode Architecture { form-width: \"30%\" }\n",
    "\n",
    "class LocalRoutingNetwork(snt.AbstractModule):\n",
    "    \"\"\"Implement neural network to deal with local routing table lookup.\n",
    "    \n",
    "    See net.in.tum.de/fileadmin/bibtex/publications/papers/geyer2018bigdama.pdf\n",
    "    figure 2 for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 query_model_fn=make_mlp_model,\n",
    "                 weight_model_fn=make_mlp_model,\n",
    "                 n_heads=3,\n",
    "                 name=\"LocalRoutingNetwork\"):\n",
    "        super(LocalRoutingNetwork, self).__init__(name=name)\n",
    "        #self._multihead_weight = []\n",
    "        with self._enter_variable_scope():\n",
    "            self._query_model = query_model_fn()\n",
    "            self._logist_routing = snt.Linear(output_size, name=\"routing_logist_layer\")\n",
    "            #for _ in range(n_heads):\n",
    "            #    self._multihead_weight.append(weight_model_fn())\n",
    "            self._model_weight = weight_model_fn()\n",
    "        \n",
    "    def _build(self, inputs, **kwargs):\n",
    "        query_output = self._query_model(inputs.globals, **kwargs)\n",
    "        queries_output = utils_tf.repeat(query_output, inputs.n_edge)\n",
    "        point_wise = tf.multiply(queries_output, inputs.edges)\n",
    "        senders_feature = tf.gather(inputs.nodes, inputs.senders)\n",
    "        weight_input = tf.concat([senders_feature, point_wise], -1)\n",
    "        #weights = []\n",
    "        #for model in self._multihead_weight:\n",
    "        #    weight_output = self._logist_routing(model(weight_input, **kwargs))\n",
    "        #    weights.append(self._unsorted_segment_softmax(\n",
    "        #        weight_output, inputs.senders, tf.reduce_sum(inputs.n_node)))\n",
    "        #output_edges = tf.reduce_mean(tf.stack(weights), axis=0)\n",
    "        output_edges = self._logist_routing(self._model_weight(weight_input, **kwargs))\n",
    "        return inputs.replace(edges=output_edges)\n",
    "    \n",
    "    def _unsorted_segment_softmax(self, x, idx, n_idx):\n",
    "        op1 = tf.exp(x)\n",
    "        op2 = tf.unsorted_segment_sum(op1, idx, n_idx)\n",
    "        op3 = tf.reduce_sum(op2, -1, keepdims=True)\n",
    "        op4 = tf.gather(op3, idx)\n",
    "        op5 = tf.divide(op1, op4)\n",
    "        return op5\n",
    "    \n",
    "class MLPGraphIndependent(snt.AbstractModule):\n",
    "    \"\"\"GraphIndependent with MLP edge, node, and global models.\"\"\"\n",
    "\n",
    "    def __init__(self, name=\"MLPGraphIndependent\"):\n",
    "        super(MLPGraphIndependent, self).__init__(name=name)\n",
    "        with self._enter_variable_scope():\n",
    "            self._network = modules.GraphIndependent(\n",
    "                edge_model_fn=make_mlp_model,\n",
    "                node_model_fn=make_mlp_model,\n",
    "                global_model_fn=None)\n",
    "\n",
    "    def _build(self, inputs, **kwargs):\n",
    "        return self._network(inputs, **kwargs)\n",
    "\n",
    "class GraphGatedNonLocalNetwork(snt.AbstractModule):\n",
    "    \"\"\"Implementation of Non-Local Neural Network. Basically, there is not used\n",
    "    global features, only ones of the nodes and edges.\n",
    "\n",
    "    See arxiv.org/abs/1806.01261 Figura 4d for more deatais about network,\n",
    "    beyond there is made the update on edge's features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 gate_recurrent_model_fn=make_gru_model,\n",
    "                 bias_shape=[LATENT_SIZE * 3],\n",
    "                 reducer=tf.unsorted_segment_sum,\n",
    "                 name=\"GraphGatedNonLocalNetwork\"):\n",
    "        \"\"\"Initializes the GraphGatedNonLocalNetwork module.\n",
    "\n",
    "        Args:\n",
    "            edge_model_fn: A callable that will be passed to EdgeBlock to perform\n",
    "                per-edge computations. The callable must return a Sonnet module (or\n",
    "                equivalent; see EdgeBlock for details).\n",
    "            node_model_fn: A callable that will be passed to NodeBlock to perform\n",
    "                per-node computations. The callable must return a Sonnet module (or\n",
    "                equivalent; see NodeBlock for details).\n",
    "            reducer: Reducer to be used by NodeBlock to aggregate nodes and edges.\n",
    "                Defaults to tf.unsorted_segment_sum.\n",
    "            name: The module name.\n",
    "        \"\"\"\n",
    "        super(GraphGatedNonLocalNetwork, self).__init__(name=name)\n",
    "\n",
    "        with self._enter_variable_scope():\n",
    "            self._edge_block = blocks.GatedEdgeBlock(\n",
    "                gate_recurrent_model_fn=gate_recurrent_model_fn,\n",
    "                use_edges=True,\n",
    "                use_receiver_nodes=True,\n",
    "                use_sender_nodes=True,\n",
    "                use_globals=False\n",
    "            )\n",
    "            self._node_block = blocks.GatedNodeBlock(\n",
    "                gate_recurrent_model_fn=gate_recurrent_model_fn,\n",
    "                bias_shape=bias_shape,\n",
    "                use_received_edges=True,\n",
    "                use_sent_edges=False,\n",
    "                use_nodes=True,\n",
    "                use_globals=False\n",
    "            )\n",
    "\n",
    "    def reset_state(self, edge_batch_size,  node_batch_size, edge_state=None, node_state=None):\n",
    "        self._edge_block.reset_state(edge_batch_size, state=edge_state)\n",
    "        self._node_block.reset_state(node_batch_size, state=node_state)\n",
    "            \n",
    "    def _build(self, graph, **kwargs):\n",
    "        \"\"\"Connects the GraphGatedNonLocalNetwork.\n",
    "\n",
    "        Args:\n",
    "          graph: A `graphs.GraphsTuple` containing `Tensor`s. The features of\n",
    "            each nodes and edges of `graph` should be concatenable on the last dimension.\n",
    "\n",
    "        Returns:\n",
    "          An output `graphs.GraphsTuple` with updated edges, nodes and globals.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._node_block(self._edge_block(graph, **kwargs), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qV1OsgBc45x6"
   },
   "outputs": [],
   "source": [
    "#@title Encode-Process-Decode Architecture { form-width: \"30%\" }\n",
    "\n",
    "class EncodeProcessDecode(snt.AbstractModule):\n",
    "    \"\"\"Full encode-process-decode model.\n",
    "\n",
    "      The model we explore includes three components:\n",
    "      - An \"Encoder\" graph net, which independently encodes the edge, node, and\n",
    "        global attributes (does not compute relations etc.).\n",
    "      - A \"Core\" graph net, which performs N rounds of processing (message-passing)\n",
    "        steps. The input to the Core is the concatenation of the Encoder's output\n",
    "        and the previous output of the Core (labeled \"Hidden(t)\" below, where \"t\" is\n",
    "        the processing step).\n",
    "      - A \"Decoder\" graph net, which independently decodes the edge, node, and\n",
    "        global attributes (does not compute relations etc.), on each message-passing\n",
    "        step.\n",
    "        \n",
    "                            h(t)        h(t + 1)\n",
    "                *---------*  |  *------*    |  *---------*    *---------*\n",
    "                |         |  |  |      |    |  |         |    |         |\n",
    "      Input --->| Encoder |  *->| Core |----*->| Decoder |--->| Lookup  |--->Output(t)\n",
    "                |         |---->|      |       |         |    |         |\n",
    "                *---------*     *------*       *---------*    *---------*\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, edge_output_size, name=\"EncodeProcessDecode\"):\n",
    "        super(EncodeProcessDecode, self).__init__(name=name)\n",
    "        with self._enter_variable_scope():\n",
    "            self._encoder = MLPGraphIndependent()\n",
    "            self._core = GraphGatedNonLocalNetwork()\n",
    "            #self._decoder = MLPGraphIndependent()\n",
    "            self._lookup = LocalRoutingNetwork(edge_output_size)\n",
    "\n",
    "    def _build(self, input_op, num_processing_steps, is_training):\n",
    "        latent0 = self._encoder(input_op, is_training=is_training)\n",
    "        latent = latent0\n",
    "        output_ops = []\n",
    "        node_batch_size = tf.reduce_sum(latent.n_node)\n",
    "        edge_batch_size = tf.reduce_sum(latent.n_edge)\n",
    "        self._core.reset_state(edge_batch_size, node_batch_size)\n",
    "        for _ in range(num_processing_steps):\n",
    "            core_input = utils_tf.concat([latent0, latent], axis=1, use_global=False)\n",
    "            latent = self._core(core_input, is_training=is_training)\n",
    "            #decoded_op = self._decoder(latent, is_training=is_training)\n",
    "            output_ops.append(self._lookup(latent, is_training=is_training))\n",
    "        return output_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gr7InKF145yA"
   },
   "source": [
    "#  Set up model training and evaluation\n",
    "\n",
    "The model we explore includes three components:\n",
    "- An \"Encoder\" graph net, which independently encodes the edge, node, and\n",
    "    global attributes (does not compute relations etc.).\n",
    "- A \"Core\" graph net, which performs N rounds of processing (message-passing)\n",
    "    steps. The input to the Core is the concatenation of the Encoder's output\n",
    "    and the previous output of the Core. Moreover the core uses a recurrent layer to\n",
    "    process the node features.\n",
    "- A \"Decoder\" graph net, which decodes the edge attributes to the output shape,\n",
    "    which is the dimension of the hot-one vector that represents with a edge (link) is\n",
    "    used to achieve a previous determined end node. This computation is made on each\n",
    "    message-passing step.\n",
    "        \n",
    "The model is trained by supervised learning. Input graphs are procedurally generated, and output\n",
    "graphs have the same structure with the edges of the shortest path labeled (using 2-element 1-hot\n",
    "vectors).\n",
    "\n",
    "The training loss is computed on the output of each processing step. The reason for this is to\n",
    "encourage the model to try to solve the problem in as few steps as possible. It also helps make\n",
    "the output of intermediate steps more interpretable.\n",
    "\n",
    "There's no need for a separate evaluate dataset because the inputs are never repeated, so the training\n",
    "loss is the measure of performance on graphs from the input distribution.\n",
    "\n",
    "We also evaluate how well the models generalize to graphs which are up to twice as large as those on which\n",
    "it was trained. The loss is computed only on the final processing step.\n",
    "\n",
    "Variables with the suffix _tr are training parameters, and variables with the suffix _ge are test/generalization\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EErmLmKV45yD",
    "outputId": "e451c36f-c32b-46d6-b8ee-13efad3a4aff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1016 01:53:02.556447 140297052448576 deprecation_wrapper.py:119] From /home/caio/.local/lib/python3.7/site-packages/graph_nets/utils_tf.py:191: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1016 01:53:02.576444 140297052448576 deprecation_wrapper.py:119] From /home/caio/.local/lib/python3.7/site-packages/sonnet/python/modules/base.py:177: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n",
      "\n",
      "W1016 01:53:02.581769 140297052448576 deprecation_wrapper.py:119] From /home/caio/.local/lib/python3.7/site-packages/sonnet/python/modules/base.py:278: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1016 01:53:02.599358 140297052448576 deprecation.py:506] From /home/caio/.local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1016 01:53:02.623425 140297052448576 deprecation.py:506] From /home/caio/.local/lib/python3.7/site-packages/sonnet/python/modules/basic.py:126: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1016 01:53:02.624325 140297052448576 deprecation.py:506] From /home/caio/.local/lib/python3.7/site-packages/sonnet/python/modules/basic.py:131: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1016 01:53:02.973050 140297052448576 deprecation.py:506] From /home/caio/.local/lib/python3.7/site-packages/sonnet/python/modules/gated_rnn.py:417: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1016 01:53:04.624813 140297052448576 deprecation.py:323] From /home/caio/.local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "/home/caio/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/caio/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W1016 01:53:05.701454 140297052448576 deprecation.py:323] From /home/caio/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:318: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "/home/caio/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/caio/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/caio/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/caio/.local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#@title Set up model { form-width: \"30%\" }\n",
    "\n",
    "tf.set_random_seed(SEED)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "random_state = np.random.RandomState(seed=SEED)\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps_tr = 2 #@param{type: 'integer'}\n",
    "num_processing_steps_ge = 2 #@param{type: 'integer'}\n",
    "\n",
    "# Data / training parameters.\n",
    "num_training_iterations = 20000 #@param{type: 'integer'}\n",
    "\n",
    "batch_size_tr = 32 #@param{type: 'integer'}\n",
    "batch_size_ge = 32 #@param{type: 'integer'}\n",
    "\n",
    "# Number of nodes per graph sampled uniformly from this range.\n",
    "min_num_nodes_tr = 8 #@param{type: 'integer'}\n",
    "max_num_nodes_tr = 20 #@param{type: 'integer'}\n",
    "min_num_nodes_ge = 16 #@param{type: 'integer'}\n",
    "max_num_nodes_ge = 33 #@param{type: 'integer'}\n",
    "num_nodes_min_max_tr = (min_num_nodes_tr, max_num_nodes_tr)\n",
    "num_nodes_min_max_ge = (min_num_nodes_ge, max_num_nodes_ge)\n",
    "\n",
    "batch_generator_tr = pybrite.graph_batch_generator(\n",
    "    batch_size_tr, num_nodes_min_max_tr, random_state=random_state)#, input_fields=dict(node=(\"pos\",)), global_field=\"pos\")\n",
    "batch_generator_ge = pybrite.graph_batch_generator(\n",
    "    batch_size_ge, num_nodes_min_max_ge, random_state=random_state)#, input_fields=dict(node=(\"pos\",)), global_field=\"pos\")\n",
    "\n",
    "# Data.\n",
    "# Input and target placeholders.\n",
    "input_ph, target_ph, weight_ph, is_training_ph = create_placeholders(batch_generator_tr)\n",
    "\n",
    "# Connect the data to the model.\n",
    "# Instantiate the model.\n",
    "model = EncodeProcessDecode(edge_output_size=1)\n",
    "# A list of outputs, one per processing step.\n",
    "output_ops_tr = model(input_ph, num_processing_steps_tr, is_training_ph)\n",
    "output_ops_ge = model(input_ph, num_processing_steps_ge, is_training_ph)\n",
    "\n",
    "# Training loss.\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr, weight_ph)\n",
    "# Loss across processing steps.\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr\n",
    "# Test/generalization loss.\n",
    "loss_ops_ge = create_loss_ops(target_ph, output_ops_ge, weight_ph)\n",
    "loss_op_ge = loss_ops_ge[-1]  # Loss from final processing step.\n",
    "\n",
    "# Optimizer.\n",
    "## Fixed\n",
    "#learning_rate = 1e-3\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#step_op = optimizer.minimize(loss_op_tr)\n",
    "## Dynamically TF Way\n",
    "starter_learning_rate = 1e-2\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#learning_rate = tf.train.cosine_decay_restarts(starter_learning_rate, global_step, first_decay_steps=150, m_mul=0.98, alpha=5e-5)\n",
    "learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step, decay_steps=15000, end_learning_rate=5e-5, power=3)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)#tf.train.MomentumOptimizer(learning_rate, momentum=0.6)\n",
    "step_op = optimizer.minimize(loss_op_tr, global_step=global_step)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs.\n",
    "input_ph, target_ph = make_all_runnable_in_session(input_ph, target_ph)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNG5-0IL45yK"
   },
   "outputs": [],
   "source": [
    "#@title Reset session  { form-width: \"30%\" }\n",
    "\n",
    "# This cell resets the Tensorflow session, but keeps the same computational\n",
    "# graph.\n",
    "\n",
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0 #@param{type: 'integer'}\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []\n",
    "\n",
    "restore_path = os.path.join(DRIVE_PATH, \"TF-GNN-W-Sess/Sess %s/\"%last_iteration)\n",
    "if os.path.isdir(restore_path):\n",
    "    saver.restore(sess, os.path.join(restore_path, \"dm.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ypocNxGZ45yR",
    "outputId": "8e744bf2-be97-464f-847b-abd3d50162e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration,Elapsed Time (s),Loss Tr,Loss Ge,Accuracy Tr,Solved Tr,Accuracy Ge,Solved Ge,True Accuracy Tr,True Solved Tr,True Accuracy Ge,True Solved Ge,False Accuracy Tr,False Solved Tr,False Accuracy Ge,False Solved Ge,Avg Batch Max Dist Diff Tr,Avg Batch Min Dist Diff Tr,Avg Batch Avg Dist Diff Tr,Avg Batch Max Hops Diff Tr,Avg Batch Min Hops Diff Tr,Avg Batch Avg Hops Diff Tr,Max Batch unreachable Tr,Min Batch unreachable Tr,Avg Batch unreachable Tr,Avg Batch Max Dist Diff Ge,Avg Batch Min Dist Diff Ge,Avg Batch Avg Dist Diff Ge,Avg Batch Max Hops Diff Ge,Avg Batch Min Hops Diff Ge,Avg Batch Avg Hops Diff Ge,Max Batch unreachable Ge,Min Batch unreachable Ge,Avg Batch unreachable Ge\n",
      "[1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio/.local/lib/python3.7/site-packages/ipykernel_launcher.py:60: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1c210ea7e2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         correct_tr, solved_tr, true_correct_tr, true_solved_tr, false_correct_tr, false_solved_tr = compute_accuracy(\n\u001b[0;32m---> 53\u001b[0;31m             train_values[\"target\"], train_values[\"outputs\"][-1])\n\u001b[0m\u001b[1;32m     54\u001b[0m         correct_ge, solved_ge, true_correct_ge, true_solved_ge, false_correct_ge, false_solved_ge = compute_accuracy(\n\u001b[1;32m     55\u001b[0m             test_values[\"target\"], test_values[\"outputs\"][-1])\n",
      "\u001b[0;32m<ipython-input-12-44948372aa84>\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(target, output, distribution)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexpect\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0macc_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0macc_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfalse_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#@title Run training  { form-width: \"30%\" }\n",
    "\n",
    "# You can interrupt this cell's training loop at any time, and visualize the\n",
    "# intermediate results by running the next cell (below). You can then resume\n",
    "# training by simply executing this cell again.\n",
    "\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 20\n",
    "\n",
    "print(\"Iteration,Elapsed Time (s),Loss Tr,Loss Ge,\"\n",
    "      \"Accuracy Tr,Solved Tr,Accuracy Ge,Solved Ge,\"\n",
    "      \"True Accuracy Tr,True Solved Tr,True Accuracy Ge,True Solved Ge,\"\n",
    "      \"False Accuracy Tr,False Solved Tr,False Accuracy Ge,False Solved Ge,\"\n",
    "      \"Avg Batch Max Dist Diff Tr,Avg Batch Min Dist Diff Tr,Avg Batch Avg Dist Diff Tr,\"\n",
    "      \"Avg Batch Max Hops Diff Tr,Avg Batch Min Hops Diff Tr,Avg Batch Avg Hops Diff Tr,\"\n",
    "      \"Max Batch unreachable Tr,Min Batch unreachable Tr,Avg Batch unreachable Tr,\"\n",
    "      \"Avg Batch Max Dist Diff Ge,Avg Batch Min Dist Diff Ge,Avg Batch Avg Dist Diff Ge,\"\n",
    "      \"Avg Batch Max Hops Diff Ge,Avg Batch Min Hops Diff Ge,Avg Batch Avg Hops Diff Ge,\"\n",
    "      \"Max Batch unreachable Ge,Min Batch unreachable Ge,Avg Batch unreachable Ge\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "    last_iteration = iteration\n",
    "    feed_dict, _ = create_feed_dict(batch_generator_tr, True, [1, 1], input_ph, target_ph, is_training_ph, weight_ph)\n",
    "    train_values = sess.run({\n",
    "        \"step\": step_op,\n",
    "        \"input\": input_ph,\n",
    "        \"target\": target_ph,\n",
    "        \"loss\": loss_op_tr,\n",
    "        \"outputs\": output_ops_tr\n",
    "    },\n",
    "        feed_dict=feed_dict)\n",
    "    \n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "    if (elapsed_since_last_log > log_every_seconds) or ((iteration + 1) % 500 == 0):\n",
    "        last_log_time = the_time\n",
    "        feed_dict, _ = create_feed_dict(batch_generator_ge, False, [1, 1], input_ph, target_ph, is_training_ph, weight_ph)\n",
    "        test_values = sess.run({\n",
    "            \"input\": input_ph,\n",
    "            \"target\": target_ph,\n",
    "            \"loss\": loss_op_ge,\n",
    "            \"outputs\": output_ops_ge\n",
    "        },\n",
    "            feed_dict=feed_dict)\n",
    "        \n",
    "        tr_path_metrics = aggregator_path_metrics(train_values[\"input\"], train_values[\"target\"], train_values[\"outputs\"][-1])\n",
    "        ge_path_metrics = aggregator_path_metrics(test_values[\"input\"], test_values[\"target\"], test_values[\"outputs\"][-1])\n",
    "\n",
    "        correct_tr, solved_tr, true_correct_tr, true_solved_tr, false_correct_tr, false_solved_tr = compute_accuracy(\n",
    "            train_values[\"target\"], train_values[\"outputs\"][-1])\n",
    "        correct_ge, solved_ge, true_correct_ge, true_solved_ge, false_correct_ge, false_solved_ge = compute_accuracy(\n",
    "            test_values[\"target\"], test_values[\"outputs\"][-1])\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        losses_tr.append(train_values[\"loss\"])\n",
    "        corrects_tr.append(correct_tr)\n",
    "        solveds_tr.append(solved_tr)\n",
    "        losses_ge.append(test_values[\"loss\"])\n",
    "        corrects_ge.append(correct_ge)\n",
    "        solveds_ge.append(solved_ge)\n",
    "        logged_iterations.append(iteration)\n",
    "        if (iteration + 1) % 500 == 0:\n",
    "            sess_path = os.path.join(DRIVE_PATH, \"TF-GNN-W-Sess/Sess %s/\"%(iteration + 1))\n",
    "            if not os.path.isdir(sess_path):\n",
    "                os.makedirs(sess_path)\n",
    "            _ = saver.save(sess, os.path.join(sess_path, \"dm.ckpt\"))\n",
    "        print(\"{:05d}, {:.1f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, \"\n",
    "              \"{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, \"\n",
    "              \"{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "                  iteration, elapsed, train_values[\"loss\"], test_values[\"loss\"], correct_tr, solved_tr, correct_ge, solved_ge,\n",
    "                  true_correct_tr, true_solved_tr, true_correct_ge, true_solved_ge,\n",
    "                  false_correct_tr, false_solved_tr, false_correct_ge, false_solved_ge,\n",
    "                  tr_path_metrics[\"avg_batch_max_dist_diff\"], tr_path_metrics[\"avg_batch_min_dist_diff\"], tr_path_metrics[\"avg_batch_avg_dist_diff\"],\n",
    "                  tr_path_metrics[\"avg_batch_max_hops_diff\"], tr_path_metrics[\"avg_batch_min_hops_diff\"], tr_path_metrics[\"avg_batch_avg_hops_diff\"],\n",
    "                  tr_path_metrics[\"max_batch_unreachable_p\"], tr_path_metrics[\"min_batch_unreachable_p\"], tr_path_metrics[\"avg_batch_unreachable_p\"],\n",
    "                  ge_path_metrics[\"avg_batch_max_dist_diff\"], ge_path_metrics[\"avg_batch_min_dist_diff\"], ge_path_metrics[\"avg_batch_avg_dist_diff\"],\n",
    "                  ge_path_metrics[\"avg_batch_max_hops_diff\"], ge_path_metrics[\"avg_batch_min_hops_diff\"], ge_path_metrics[\"avg_batch_avg_hops_diff\"],\n",
    "                  ge_path_metrics[\"max_batch_unreachable_p\"], ge_path_metrics[\"min_batch_unreachable_p\"], ge_path_metrics[\"avg_batch_unreachable_p\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "routing-new-metrics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
